# Prototype extract/transform script from GTEx v7 metadata to draft C2M2 core metadata standard

## Contents

This directory contains the prototype GTEx extractor script along with
- a `Table-Schema` JSON file describing the output (subfolder `000`)
- raw input data from GTEx (subfolder `001`; this contains two files prepared specially by GTEx people for last year's DCPPC round; replication on live data will require changes in GTEx's access model; see first item under 'Design notes that will definitely need to be addressed', below)
- some auxiliary files (subfolder `002`) mapping GTEx terminology to terms in selected controlled vocabularies (cached in subfolder `003`)
- a gzipped tarball containing example output (file `GTEx*tgz`; this is a bdbag of (a renamed copy of) subfolder `004`)

## Dependencies

- A working version of Python3
- `bdbag` needs to be accessible via `$PATH`

## Design notes that will definitely need to be addressed

- Simplifying constraint: this implementation is agnostic to public/private data distinctions, a deficiency which is going to need to be resolved downstream, at least for GTEx. As far as I can tell, all but the most superficial of their actual data files (e.g. sequence FASTQ, CRAM alignment reports) are protected, as is information regarding names and access locations/URLs for those files. The file location data I'm extracting with this prototype script was specifically dumped by GTEx people last year to facilitate a prior version of our ingest attempt: consultation will need to be done with GTEx to figure out a formal way of indexing files (down to their names, which don't contain protected info, and hopefully URLs) for search/discovery while preserving whatever data-protection gatekeepers need to remain in place between users and the files themselves.

- I've created a basic hierarchy of `Dataset`s by hand (top_level -> alignment-file_data -> { RNA-Seq alignment files, WGS alignment files}). There are too many different possibilities for slicing this data into subsets to yield an obvious automatic solution to Dataset construction: consultation will need to happen at some level to establish, in advance, a map from GTEx logical containment structures (from "nucleic acid isolation batch ID" all the way up to "version 7 data release") to C2M2 Datasets. Similarly, decisions on `SubjectGroup`s will need to be made; right now I've just dumped all `Subject`s into a single group to model the structure itself.

- Since (according to the current model) `BioSample`s are `AssayedBy` `DataEvent`s which produce `File`s which are in turn `AnalyzedBy` `DataEvent`s which then produce further downstream `File`s, I've had to fill in dummy `File` data representing sequence FASTQ to create the minimal production chain the model requires. All I have right now are locations of CRAM files describing read alignments to a human reference for both RNA-Seq and WGS reads. To model these properly, they had to be generated by alignment `DataEvent`s consuming read-sequence `File`s to produce the relevant alignment `File`s -- so I made up some interstitial sequence `File`s and dumped them (with fake URLs and associated `DataEvent` instances) into the output data. See first bullet in this list for a resolution sketch.

## Design notes that may or may not need to be addressed

- The Table-Schema JSON included here implements the Frictionless Data "tabular data package" spec (https://frictionlessdata.io/specs/tabular-data-package/), which is a container for instances of the "table schema" object spec (https://frictionlessdata.io/specs/table-schema/). Web and script validators are available; the schema currently passes the definition constraints (except that my version allows uppercase letters in table names, while the canonical schema doesn't), and the tables pass validation checks vs. their associated schematic definitions.

- Serialization format: a collection of TSVs is built by the script, representing table data defined in my version of the draft core metadata model.

- The `Protocol` structure is currently modeled as a stub: I have no reference data to link to. The idea seems to require references to a preexisting (curated) collection of method descriptions, which as far as I know doesn't exist yet.

Arthur Brady / IGS
