# Prototype extract/transform script from GTEx v7 metadata to draft C2M2 core metadata standard

## Contents

This directory contains the prototype GTEx extractor script along with
- a `Table-Schema` JSON file describing the output
- raw input data from GTEx
- some auxiliary files mapping GTEx terminology to terms in selected controlled vocabularies
- a gzipped tarball containing example output
- an ER diagram (based on but different from Karl's working draft) precisely describing output structure
  - ...except for DCC-specific auxiliary data, which isn't drawn: this is encoded as a flat table (`AuxiliaryData.tsv`) of 4-tuples: `ObjectType`, `ObjectID`, `DataDescription` and `Value`, so that each record generically links some bit of metadata to some existing object \[like a `BioSample` or a `Subject`\] within the C2M2 model).

## Dependencies

- A working version of Perl5
- `bdbag` needs to be accessible via `$PATH`

## Design notes that will definitely need to be addressed

- Simplifying constraint: this implementation is agnostic to public/private data distinctions, a deficiency which is going to need to be resolved downstream, at least for GTEx. As far as I can tell, all but the most superficial of their actual data files (e.g. sequence FASTQ, CRAM alignment reports) are protected, as is information regarding names and access locations/URLs for those files. The file location data I'm extracting with this prototype script was specifically dumped by GTEx people last year to facilitate a prior version of our ingest attempt: consultation will need to be done with GTEx to figure out a formal way of indexing files (down to their names, which don't contain protected info, and hopefully URLs) for search/discovery while preserving whatever data-protection gatekeepers need to remain in place between users and the files themselves.

- I've created a basic hierarchy of `Dataset`s by hand (top_level -> alignment-file_data -> { RNA-Seq alignment files, WGS alignment files}). There are too many different possibilities for slicing this data into subsets to yield an obvious automatic solution to Dataset construction: consultation will need to happen at some level to establish, in advance, a map from GTEx logical containment structures (from "nucleic acid isolation batch ID" all the way up to "version 7 data release") to C2M2 Datasets. Similarly, decisions on `SubjectGroup`s will need to be made; right now I've just dumped all `Subject`s into a single group to model the structure itself.

- Since (according to the current model) `BioSample`s are `AssayedBy` `DataEvent`s which produce `File`s which are in turn `AnalyzedBy` `DataEvent`s which then produce further downstream `File`s, I've had to fill in dummy `File` data representing sequence FASTQ to create the minimal production chain the model requires. All I have right now are locations of CRAM files describing read alignments to a human reference for both RNA-Seq and WGS reads. To model these properly, they had to be generated by alignment `DataEvent`s consuming read-sequence `File`s to produce the relevant alignment `File`s -- so I made up some interstitial sequence `File`s and dumped them (with fake URLs) into the output data. See first bullet in this list for a resolution sketch.

- The script needs to be translated into Python3. I plan to get this done by 13 September at the latest, possibly sooner. The hope is that this exercise will increase my Python development rate (which is currently much worse than for Perl), but in the interests of efficiency and momentum, I'm also open to someone jumping in and doing it for me. Please send me a direct message if you're interested.

## Design notes that may or may not need to be addressed

- External CVs currently in use:
  - OBI (BioSample.sample_type, DataEvent.method)
  - EDAM (File.information_type, File.file_format)
  - NCBI Taxonomy (SubjectTaxonomy.NCBITaxonID)
  
  These are currently implemented as fields containing full URLs referencing the relevant terms.

- The Table-Schema implements the Frictionless Data "tabular data package" spec (https://frictionlessdata.io/specs/tabular-data-package/), which contains instances of that group's "table schema" object (https://frictionlessdata.io/specs/table-schema/).

- I've abandoned JSON as a serialization format: a collection of TSVs is built instead, representing table data defined in my version of the draft core metadata model (included here as a PNG).

- This prototype is agnostic to DATS compliance (i.e., it's noncompliant), but it can be made compliant with relatively little pain. Anyone who feels strongly about this issue should begin a conversation about it within the group.

- I've removed a bunch of `id` and `url` fields from Karl's latest draft ER model where they seemed redundant; I also removed `synonyms` fields (largely due to a lack of anything to populate them with, not a principled objection to their existence).

- The `Protocol` structure is currently modeled as a stub: I have no reference data to link to. Seems to require a preexisting (curated) collection of method descriptions, which doesn't exist yet as far as I know.

I believe all other differences between my model (see diagram in this folder) and Karl's current draft model (https://github.com/nih-cfde/cfde-deriva/blob/master/diagrams/cfde-core-model.png) are trivially self-explanatory; if there's something I missed, hit me up and I'll expand this document.

Arthur Brady
IGS
